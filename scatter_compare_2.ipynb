{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "013da3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9eade",
   "metadata": {},
   "source": [
    "### Focusing on One Data File - First Lets Write the Code to Just find PEAKMJD in a file ###\n",
    "\n",
    "`PEAKMJD` is the Modified Julian Date (MJD) at which the supernova reaches peak brightness.\n",
    "It is listed in the header of each `.DAT` file.\n",
    "\n",
    "MJD is a  version of the Julian Date (JD) designed to use smaller numbers and to start the day at midnight instead of noon.\n",
    "\n",
    "The relationship is:\n",
    "\n",
    "$\n",
    "\\text{MJD} = \\text{JD} - 2400000.5\n",
    "$\n",
    "\n",
    "Julian Date (JD): Counts days continuously from January 1, 4713 BCE at 12:00 TT (Julian calendar).\n",
    "\n",
    "TT stands for **Terrestrial Time**, a modern astronomical time standard used for calcualtions.\n",
    "\n",
    "\n",
    "\n",
    "<h3><u>Lets look at a single file:</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa71a497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEAKMJD: 61800.129\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/pittsburghgraduatestudent/repos/pippin_learn/MC_BAYSN_NOSCATTER/MC_BAYSN_NOSCATTER_SN000001.DAT\"\n",
    "\n",
    "peak_mjd = None\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if \"PEAKMJD\" in line:\n",
    "            parts = line.strip().replace(\":\", \"\").split()\n",
    "            if \"PEAKMJD\" in parts:\n",
    "                peak_mjd = float(parts[parts.index(\"PEAKMJD\") + 1])\n",
    "                break\n",
    "\n",
    "print(f\"PEAKMJD: {peak_mjd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c41b87",
   "metadata": {},
   "source": [
    "### Now lets look at that same single file and see if we can just filter out all the band measurements taht fall outside the +/- 7 days around the PEAKMJD ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610aa3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(61794.0825, 'LSST-r', 132721.0), (61795.0967, 'LSST-g', 168515.0), (61795.1033, 'LSST-i', 109874.0), (61795.113, 'LSST-r', 150376.0), (61800.1048, 'LSST-r', 199163.0), (61800.1287, 'LSST-g', 219159.0), (61802.0967, 'LSST-g', 218760.0), (61802.1033, 'LSST-i', 111339.0), (61802.113, 'LSST-r', 199301.0), (61808.0995, 'LSST-r', 162949.0), (61808.1233, 'LSST-i', 82502.5), (61813.0993, 'LSST-g', 114577.0), (61813.1059, 'LSST-i', 63145.6), (61813.1156, 'LSST-r', 121951.0), (61814.0297, 'LSST-i', 61825.9), (61817.0707, 'LSST-g', 81181.7), (61817.0773, 'LSST-i', 62970.9), (61817.0871, 'LSST-r', 103446.0), (61819.0178, 'LSST-i', 65964.3), (61822.1866, 'LSST-g', 50725.9), (61823.0234, 'LSST-i', 70918.1), (61824.0354, 'LSST-g', 42799.3), (61824.042, 'LSST-i', 69316.4), (61824.0517, 'LSST-r', 88240.5), (61827.0407, 'LSST-g', 34281.9), (61827.0645, 'LSST-r', 76913.4), (61832.0284, 'LSST-g', 25747.6), (61832.035, 'LSST-i', 52246.9), (61832.0448, 'LSST-r', 57004.6), (61835.0845, 'LSST-i', 43953.6), (61835.1083, 'LSST-r', 48272.7)]\n",
      "PEAKMJD: 61800.129\n",
      "Number of near-peak rows: 9\n",
      "          MJD    BAND   FLUXCAL\n",
      "0  61794.0825  LSST-r  132721.0\n",
      "1  61795.0967  LSST-g  168515.0\n",
      "2  61795.1033  LSST-i  109874.0\n",
      "3  61795.1130  LSST-r  150376.0\n",
      "4  61800.1048  LSST-r  199163.0\n",
      "5  61800.1287  LSST-g  219159.0\n",
      "6  61802.0967  LSST-g  218760.0\n",
      "7  61802.1033  LSST-i  111339.0\n",
      "8  61802.1130  LSST-r  199301.0\n"
     ]
    }
   ],
   "source": [
    "# New data rows that will be stored once filtering is done.\n",
    "data_rows = []\n",
    "\n",
    "# Opens file and reads each line.\n",
    "# note: parts[1] is MJD, parts[2] is BAND, parts[4] is FLUXCAL\n",
    "# data_rows is a list of tuples (MJD, BAND, FLUXCAL)\n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"OBS:\"):\n",
    "            parts = line.strip().split() #strips spaces, splits into python list.\n",
    "            mjd = float(parts[1])\n",
    "            band = parts[2]\n",
    "            fluxcal = float(parts[4])\n",
    "            data_rows.append((mjd, band, fluxcal))\n",
    "\n",
    "# --- Filter rows within ±7 days of PEAKMJD ---\n",
    "# note: row[0] is the first entry in tuple\n",
    "filtered_rows = []\n",
    "for row in data_rows:\n",
    "    mjd_value = row[0]\n",
    "    time_diff = abs(mjd_value - peak_mjd) \n",
    "    if time_diff <= 7:\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "# --- Convert to a DataFrame ---\n",
    "df_filtered = pd.DataFrame(filtered_rows, columns=[\"MJD\", \"BAND\", \"FLUXCAL\"])\n",
    "\n",
    "print(f\"PEAKMJD: {peak_mjd}\")\n",
    "print(f\"Number of near-peak rows: {len(df_filtered)}\")\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf074cd",
   "metadata": {},
   "source": [
    "### Now lets see if we can filter out anything that is within +/-7 days of the peak MJD ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e65009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEAKMJD: 61800.129\n",
      "Number of near-peak rows: 9\n",
      "          MJD    BAND FIELD   FLUXCAL  FLUXCALERR  PHOTFLAG  GAIN    ZPT  \\\n",
      "0  61794.0825  LSST-r   DDF  132721.0      5.2012      4096   1.0  36.73   \n",
      "1  61795.0967  LSST-g   DDF  168515.0      2.7386      6144   1.0  38.38   \n",
      "2  61795.1033  LSST-i   DDF  109874.0      1.5482      4096   1.0  39.16   \n",
      "3  61795.1130  LSST-r   DDF  150376.0      1.7502      4096   1.0  39.23   \n",
      "4  61800.1048  LSST-r   DDF  199163.0      6.4314      4096   1.0  36.71   \n",
      "5  61800.1287  LSST-g   DDF  219159.0      7.1556      4096   1.0  36.58   \n",
      "6  61802.0967  LSST-g   DDF  218760.0      3.1493      4096   1.0  38.36   \n",
      "7  61802.1033  LSST-i   DDF  111339.0      1.5759      4096   1.0  39.14   \n",
      "8  61802.1130  LSST-r   DDF  199301.0      2.0444      4096   1.0  39.20   \n",
      "\n",
      "    PSF  SKY_SIG  SIM_MAGOBS  \n",
      "0  1.60    243.8     14.6835  \n",
      "1  1.59    332.6     14.4213  \n",
      "2  1.38   1156.0     14.8982  \n",
      "3  1.49    786.2     14.5610  \n",
      "4  2.02    246.5     14.2556  \n",
      "5  2.03    148.5     14.1452  \n",
      "6  1.89    334.2     14.1539  \n",
      "7  1.73   1166.0     14.8767  \n",
      "8  2.13    790.1     14.2460  \n"
     ]
    }
   ],
   "source": [
    "# New data rows that will be stored once filtering is done.\n",
    "# Each entry will be a tuple in the same order as VARLIST:\n",
    "# (MJD, BAND, FIELD, FLUXCAL, FLUXCALERR, PHOTFLAG, GAIN, ZPT, PSF, SKY_SIG, SIM_MAGOBS)\n",
    "data_rows = []\n",
    "\n",
    "# Opens file and reads each line.\n",
    "# note: parts[1] is MJD, parts[2] is BAND, parts[3] is FIELD, parts[4] is FLUXCAL, etc.\n",
    "# note: MJD is measured in days. \n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"OBS:\"):\n",
    "            parts = line.strip().split()  # strips spaces, splits into python list.\n",
    "\n",
    "            # Extract variables in the same order as VARLIST\n",
    "            mjd = float(parts[1])\n",
    "            band = parts[2]\n",
    "            field = parts[3]\n",
    "            fluxcal = float(parts[4])\n",
    "            fluxcalerr = float(parts[5])\n",
    "            photflag = int(parts[6])\n",
    "            gain = float(parts[7])\n",
    "            zpt = float(parts[8])\n",
    "            psf = float(parts[9])\n",
    "            sky_sig = float(parts[10])\n",
    "            sim_magobs = float(parts[11])\n",
    "\n",
    "            # Store everything as a list of tuples.\n",
    "            # Each tuple same as VARLIST order in og doc.\n",
    "            data_rows.append((\n",
    "                mjd, band, field, fluxcal, fluxcalerr,\n",
    "                photflag, gain, zpt, psf, sky_sig, sim_magobs\n",
    "            ))\n",
    "\n",
    "# --- Filter rows within ±7 days of PEAKMJD ---\n",
    "# note: row[0] is the first entry in tuple (MJD)\n",
    "filtered_rows = []\n",
    "for row in data_rows:\n",
    "    mjd_value = row[0]\n",
    "    time_diff = abs(mjd_value - peak_mjd)\n",
    "    if time_diff <= 7:\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "# --- Convert to a DataFrame ---\n",
    "# Columns are labeled according to the VARLIST from the .DAT file\n",
    "df_filtered = pd.DataFrame(\n",
    "    filtered_rows,\n",
    "    columns=[\n",
    "        \"MJD\", \"BAND\", \"FIELD\", \"FLUXCAL\", \"FLUXCALERR\",\n",
    "        \"PHOTFLAG\", \"GAIN\", \"ZPT\", \"PSF\", \"SKY_SIG\", \"SIM_MAGOBS\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"PEAKMJD: {peak_mjd}\")\n",
    "print(f\"Number of near-peak rows: {len(df_filtered)}\")\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbffca81",
   "metadata": {},
   "source": [
    "### Now lets automate this to all the .dat files out there ###\n",
    "\n",
    "Step 1 would be to define a method that collects all the near peak rows froma. given folder (i.e. MC_BAYSN_NOSCATTER or MC_BAYSN_PLUSSCATTER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_near_peak_rows(folder_path):\n",
    "    \"\"\"\n",
    "    Collects all near-peak (±7 days) rows from all .DAT files in a folder.\n",
    "    Returns a DataFrame with all VARLIST columns + FILENAME.\n",
    "    \"\"\"\n",
    "\n",
    "    # Empty list to store all rows from all files\n",
    "    all_rows = []\n",
    "\n",
    "    # Loops through each file in the specified folder\n",
    "    # and builds path to each .DAT file\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".DAT\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "\n",
    "            # --- Extract PEAKMJD ---\n",
    "            peak_mjd = None # Emtpy variable to store PEAKMJD\n",
    "            with open(file_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    if \"PEAKMJD\" in line:\n",
    "                        parts = line.strip().replace(\":\", \"\").split()\n",
    "                        peak_mjd = float(parts[parts.index(\"PEAKMJD\") + 1])\n",
    "                        break\n",
    "\n",
    "            # --- Extract OBS rows and filter by ±7 days ---\n",
    "            with open(file_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    if line.startswith(\"OBS:\"):\n",
    "                        parts = line.strip().split()\n",
    "                        mjd = float(parts[1])\n",
    "                        band = parts[2]\n",
    "                        field = parts[3]\n",
    "                        fluxcal = float(parts[4])\n",
    "                        fluxcalerr = float(parts[5])\n",
    "                        photflag = int(parts[6])\n",
    "                        gain = float(parts[7])\n",
    "                        zpt = float(parts[8])\n",
    "                        psf = float(parts[9])\n",
    "                        sky_sig = float(parts[10])\n",
    "                        sim_magobs = float(parts[11])\n",
    "\n",
    "                        ## all_rows built up sequentially as we read each line.\n",
    "                        if abs(mjd - peak_mjd) <= 7:\n",
    "                            all_rows.append((\n",
    "                                filename, mjd, band, field, fluxcal, fluxcalerr,\n",
    "                                photflag, gain, zpt, psf, sky_sig, sim_magobs\n",
    "                            ))\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        all_rows,\n",
    "        columns=[\n",
    "            \"FILENAME\", \"MJD\", \"BAND\", \"FIELD\", \"FLUXCAL\", \"FLUXCALERR\",\n",
    "            \"PHOTFLAG\", \"GAIN\", \"ZPT\", \"PSF\", \"SKY_SIG\", \"SIM_MAGOBS\"\n",
    "        ]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d667e6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOSCATTER near-peak rows: 359\n",
      "PLUSSCATTER near-peak rows: 363\n",
      "NOSCATTER data saved to nos_near_peak_rows.csv\n",
      "PLUSSCATTER data saved to plus_near_peak_rows.csv\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: Collect rows from both folders ---\n",
    "nos_folder = \"/Users/pittsburghgraduatestudent/repos/pippin_learn/MC_BAYSN_NOSCATTER\"\n",
    "plus_folder = \"/Users/pittsburghgraduatestudent/repos/pippin_learn/MC_BAYSN_PLUSSCATTER\"\n",
    "\n",
    "nos_df = collect_near_peak_rows(nos_folder)\n",
    "plus_df = collect_near_peak_rows(plus_folder)\n",
    "\n",
    "print(f\"NOSCATTER near-peak rows: {len(nos_df)}\")\n",
    "print(f\"PLUSSCATTER near-peak rows: {len(plus_df)}\")\n",
    "\n",
    "nos_df.to_csv(\"nos_near_peak_rows.csv\", index=False)\n",
    "plus_df.to_csv(\"plus_near_peak_rows.csv\", index=False)\n",
    "\n",
    "print(\"NOSCATTER data saved to nos_near_peak_rows.csv\")\n",
    "print(\"PLUSSCATTER data saved to plus_near_peak_rows.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e31a120",
   "metadata": {},
   "source": [
    "### Sorted by FILENAME and MJD to Better Explore the Code Using Human Eyes ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c490c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted NOSCATTER data saved to sorted_nos_near_peak_rows.csv\n",
      "Sorted PLUSSCATTER data saved to sorted_plus_near_peak_rows.csv\n"
     ]
    }
   ],
   "source": [
    "# Keep nos_df unchanged, create a sorted copy\n",
    "sorted_nos_df = nos_df.sort_values(by=[\"FILENAME\", \"MJD\"]).reset_index(drop=True)\n",
    "\n",
    "# Save sorted DataFrame to CSV\n",
    "sorted_nos_df.to_csv(\"sorted_nos_near_peak_rows.csv\", index=False)\n",
    "print(\"Sorted NOSCATTER data saved to sorted_nos_near_peak_rows.csv\")\n",
    "\n",
    "# Keep plus_df unchanged, create a sorted copy\n",
    "sorted_plus_df = plus_df.sort_values(by=[\"FILENAME\", \"MJD\"]).reset_index(drop=True)\n",
    "\n",
    "# Save sorted DataFrame to CSV\n",
    "sorted_plus_df.to_csv(\"sorted_plus_near_peak_rows.csv\", index=False)\n",
    "print(\"Sorted PLUSSCATTER data saved to sorted_plus_near_peak_rows.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec01ea",
   "metadata": {},
   "source": [
    "### Found Out the cause of the Mismatch ###\n",
    "\n",
    "The individual FILENAME of each row, as part of the following .csv files contain `NOSCATTER` and `PLUSSCATTER` in them. Making it impossible to match any rows together even if matches exist. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6cd306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to merged_near_peak_rows.csv\n"
     ]
    }
   ],
   "source": [
    "nos_df[\"FILENAME\"] = nos_df[\"FILENAME\"].str.extract(r\"(SN\\d+)\")\n",
    "plus_df[\"FILENAME\"] = plus_df[\"FILENAME\"].str.extract(r\"(SN\\d+)\")\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    nos_df,\n",
    "    plus_df,\n",
    "    on=[\"FILENAME\", \"MJD\", \"BAND\"],\n",
    "    suffixes=(\"_NO_S\", \"_PLUS_S\"),\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "merged_df.to_csv(\"merged_near_peak_rows.csv\", index=False)\n",
    "print(\"Merged data saved to merged_near_peak_rows.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c01fe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows only in NOSCATTER (left_only):\n",
      "     FILENAME         MJD    BAND\n",
      "85   SN000016  60990.1218  LSST-r\n",
      "132  SN000025  61474.0242  LSST-g\n",
      "133  SN000025  61474.0308  LSST-i\n",
      "155  SN000038  60876.3473  LSST-g\n",
      "156  SN000038  60876.3550  LSST-i\n",
      "157  SN000038  60876.3647  LSST-r\n",
      "\n",
      "Rows only in PLUSSCATTER (right_only):\n",
      "     FILENAME         MJD    BAND\n",
      "124  SN000025  61456.0911  LSST-r\n",
      "125  SN000025  61456.1147  LSST-i\n",
      "158  SN000038  60886.2919  LSST-r\n",
      "159  SN000038  60886.3158  LSST-i\n",
      "160  SN000038  60890.2040  LSST-g\n",
      "161  SN000038  60890.2116  LSST-i\n",
      "162  SN000038  60890.2213  LSST-r\n",
      "326  SN000086  61379.1589  LSST-g\n",
      "327  SN000086  61379.1655  LSST-i\n",
      "328  SN000086  61379.1752  LSST-r\n"
     ]
    }
   ],
   "source": [
    "compare_df = pd.merge(\n",
    "    nos_df,\n",
    "    plus_df,\n",
    "    on=[\"FILENAME\", \"MJD\", \"BAND\"],\n",
    "    suffixes=(\"_NO_S\", \"_PLUS_S\"),\n",
    "    how=\"outer\",\n",
    "    indicator=True  # Adds a column showing merge source\n",
    ")\n",
    "\n",
    "# --- Extract missing rows  NOSCATTER  PLUSSCATTER ---\n",
    "left_only = compare_df[compare_df[\"_merge\"] == \"left_only\"]\n",
    "right_only = compare_df[compare_df[\"_merge\"] == \"right_only\"]\n",
    "\n",
    "# --- Print results ---\n",
    "print(\"\\nRows only in NOSCATTER (left_only):\")\n",
    "print(left_only[[\"FILENAME\", \"MJD\", \"BAND\"]])\n",
    "\n",
    "print(\"\\nRows only in PLUSSCATTER (right_only):\")\n",
    "print(right_only[[\"FILENAME\", \"MJD\", \"BAND\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43c725",
   "metadata": {},
   "source": [
    "### Lets Print Out What Did Not Get Merged ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94147fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
